{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify PCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify PCI skills via fuzzy matching\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "df1 = pd.read_parquet(r'import1.parquet')\n",
    "df2 = pd.read_parquet(r'import2.parquet')\n",
    "\n",
    "keywords = df2['PCI_keywords'].dropna().unique()\n",
    "\n",
    "# Fuzzy matching function\n",
    "def fuzzy_match(skill_mapped):\n",
    "    if pd.isna(skill_mapped):\n",
    "        return np.nan, 0.0\n",
    "    scores = [(kw, SequenceMatcher(None, str(skill_mapped).lower(), str(kw).lower()).ratio()) \n",
    "              for kw in keywords]\n",
    "    best_kw, best_score = max(scores, key=lambda x: x[1])\n",
    "    return best_kw, best_score\n",
    "\n",
    "tqdm.pandas(desc=\"Fuzzy Matching Progress\")\n",
    "\n",
    "df1[['PCI_keywords', 'skills_match_score']] = df1['skill_mapped'].progress_apply(\n",
    "    lambda x: pd.Series(fuzzy_match(x))\n",
    ")\n",
    "\n",
    "print(f\"\\nMatching completed! Statistics:\")\n",
    "print(f\"Max match score: {df1['skills_match_score'].max():.4f}\")\n",
    "print(f\"Min match score: {df1['skills_match_score'].min():.4f}\")\n",
    "print(f\"Average match score: {df1['skills_match_score'].mean():.4f}\")\n",
    "print(f\"\\nTop 5 matching results:\")\n",
    "print(df1[['skill_mapped', 'PCI_keywords', 'skills_match_score']].head())\n",
    "\n",
    "df1.to_parquet(r'output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify PCI skills\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet(\n",
    "    \"imput.parquet\",\n",
    "    columns=['user_id', 'skills_match_score', 'is_ai_broad']\n",
    ")\n",
    "\n",
    "# Add PCI skill flag column\n",
    "df1['is_pci_skill'] = (df1['skills_match_score'] >= 0.7).astype(int)\n",
    "\n",
    "# Drop unused column\n",
    "df1 = df1.drop(columns=['skills_match_score'])\n",
    "\n",
    "df1 = df1.drop_duplicates(ignore_index=True)\n",
    "\n",
    "df1.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify PCI employees\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet(\"imput.parquet\")\n",
    "\n",
    "df1['is_pci'] = (df1['pci_score'] >= 0.3).astype(int)\n",
    "\n",
    "df1 = df1[['user_id', 'is_pci', 'is_ai_broad']]\n",
    "df1 = df1.drop_duplicates()\n",
    "\n",
    "df1.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AI employees\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "df1 = pd.read_parquet(\"imput1.parquet\")\n",
    "df2 = pd.read_excel(\"imput2.xlsx\")\n",
    "\n",
    "keywords = df2['AI_keywords'].dropna().unique()\n",
    "\n",
    "# Fuzzy matching function\n",
    "def fuzzy_match(skill_mapped):\n",
    "    if pd.isna(skill_mapped):\n",
    "        return np.nan, 0.0\n",
    "    scores = [(kw, SequenceMatcher(None, str(skill_mapped).lower(), str(kw).lower()).ratio()) \n",
    "              for kw in keywords]\n",
    "    best_kw, best_score = max(scores, key=lambda x: x[1])\n",
    "    return best_kw, best_score\n",
    "\n",
    "tqdm.pandas(desc=\"Fuzzy Matching Progress\")\n",
    "\n",
    "df1[['AI_keywords', 'ai_skills_match_score']] = df1['skill_mapped'].progress_apply(\n",
    "    lambda x: pd.Series(fuzzy_match(x))\n",
    ")\n",
    "\n",
    "print(f\"\\nMatching completed! Statistics:\")\n",
    "print(f\"Max match score: {df1['ai_skills_match_score'].max():.4f}\")\n",
    "print(f\"Min match score: {df1['ai_skills_match_score'].min():.4f}\")\n",
    "print(f\"Average match score: {df1['ai_skills_match_score'].mean():.4f}\")\n",
    "print(f\"\\nTop 5 matching results:\")\n",
    "print(df1[['skill_mapped', 'AI_keywords', 'ai_skills_match_score']].head())\n",
    "\n",
    "df1.to_parquet(r'output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AI employees\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet(\n",
    "    \"imput.parquet\",\n",
    "    columns=['user_id', 'ai_skills_match_score']\n",
    ")\n",
    "\n",
    "df1['is_ai_broad'] = (df1['ai_skills_match_score'] >= 0.75).astype(int)\n",
    "\n",
    "df1 = df1.drop(columns=['ai_skills_match_score'])\n",
    "\n",
    "df1 = df1.drop_duplicates(ignore_index=True)\n",
    "\n",
    "df1.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify AIPCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AIPCI employees\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet(\"imput.parquet\")\n",
    "\n",
    "df1['is_ai_pci'] = ((df1['is_pci'] == 1) & (df1['is_ai_broad'] == 1)).astype(int)\n",
    "\n",
    "target_columns = ['is_pci', 'is_ai_broad', 'is_ai_pci']\n",
    "for col in target_columns:\n",
    "    print(f\"=== {col}\")\n",
    "    unique_user_count = df1.groupby(col)['user_id'].nunique()\n",
    "    print(unique_user_count)\n",
    "    print()\n",
    "    \n",
    "df1.to_parquet(r'output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate AI/PCI/AIPCI share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AI/PCI/AIPCI workforce share\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet(\"imput.parquet\")[\n",
    "    ['rcid', 'year_quarter', 'PCI_workers_it', 'AI_workers_it', 'AIPCI_workers_it', 'total_workforce_it']\n",
    "]\n",
    "\n",
    "# Calculate share columns (threshold = 2)\n",
    "df1['AI_workforce_share_it_2'] = df1.apply(\n",
    "    lambda x: x['AI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 2 else 0, axis=1\n",
    ")\n",
    "df1['PCI_workforce_share_it_2'] = df1.apply(\n",
    "    lambda x: x['PCI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 2 else 0, axis=1\n",
    ")\n",
    "df1['AIPCI_workforce_share_it_2'] = df1.apply(\n",
    "    lambda x: x['AIPCI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 2 else 0, axis=1\n",
    ")\n",
    "\n",
    "# Calculate share columns (threshold = 5)\n",
    "df1['AI_workforce_share_it_5'] = df1.apply(\n",
    "    lambda x: x['AI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 5 else 0, axis=1\n",
    ")\n",
    "df1['PCI_workforce_share_it_5'] = df1.apply(\n",
    "    lambda x: x['PCI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 5 else 0, axis=1\n",
    ")\n",
    "df1['AIPCI_workforce_share_it_5'] = df1.apply(\n",
    "    lambda x: x['AIPCI_workers_it'] / x['total_workforce_it'] if x['total_workforce_it'] >= 5 else 0, axis=1\n",
    ")\n",
    "\n",
    "df1.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set initial stock $C_{i,0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"imput.parquet\")\n",
    "\n",
    "# Define variable pairs for initial stock calculation\n",
    "init_var_pairs = [\n",
    "    ('AI_workforce_share_it_2', 'AIC_init_stock_2'),\n",
    "    ('PCI_workforce_share_it_2', 'PCIC_init_stock_2'),\n",
    "    ('AIPCI_workforce_share_it_2', 'AIPCI_init_stock_2'),\n",
    "    ('AI_workforce_share_it_5', 'AIC_init_stock_5'),\n",
    "    ('PCI_workforce_share_it_5', 'PCIC_init_stock_5'),\n",
    "    ('AIPCI_workforce_share_it_5', 'AIPCI_init_stock_5')\n",
    "]\n",
    "\n",
    "# Calculate and merge initial stock for each firm\n",
    "for centered_stock_var, init_stock_var in init_var_pairs:\n",
    "    def get_first_non_null(group):\n",
    "        non_null_vals = group[centered_stock_var].dropna()\n",
    "        if not non_null_vals.empty:\n",
    "            return non_null_vals.iloc[0]\n",
    "        else:\n",
    "            return group[centered_stock_var].mean()\n",
    "    \n",
    "    init_stock_df = df.groupby('rcid').apply(\n",
    "        get_first_non_null,\n",
    "        include_groups=False\n",
    "    ).reset_index()\n",
    "    init_stock_df.columns = ['rcid', init_stock_var]\n",
    "    df = pd.merge(df, init_stock_df, on='rcid', how='left')\n",
    "\n",
    "df.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate stock using estimated œÅ and k parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_parquet(\"imput.parquet\")\n",
    "df2 = pd.read_excel(\"imput.xlsx\")\n",
    "\n",
    "# Parameter mapping and column definition\n",
    "suffix = \"_œÅk\"  \n",
    "model_param_pairs = [\n",
    "    (('AI_workforce_share_it_2', 'AI_post_share_i,t-1_2', 'AIC_init_stock_2'), 0, 'AI_2', f'AIC_stock_it_2{suffix}'),\n",
    "    (('PCI_workforce_share_it_2', 'PCI_post_share_i,t-1_2', 'PCIC_init_stock_2'), 1, 'PCI_2', f'PCIC_stock_it_2{suffix}'),\n",
    "    (('AIPCI_workforce_share_it_2', 'AIPCI_post_share_i,t-1_2', 'AIPCI_init_stock_2'), 2, 'AIPCI_2', f'AIPCI_stock_it_2{suffix}'),\n",
    "    (('AI_workforce_share_it_5', 'AI_post_share_i,t-1_5', 'AIC_init_stock_5'), 3, 'AI_5', f'AIC_stock_it_5{suffix}'),\n",
    "    (('PCI_workforce_share_it_5', 'PCI_post_share_i,t-1_5', 'PCIC_init_stock_5'), 4, 'PCI_5', f'PCIC_stock_it_5{suffix}'),\n",
    "    (('AIPCI_workforce_share_it_5', 'AIPCI_post_share_i,t-1_5', 'AIPCI_init_stock_5'), 5, 'AIPCI_5', f'AIPCI_stock_it_5{suffix}'),\n",
    "]\n",
    "\n",
    "# Print estimated parameters\n",
    "param_print_format = [\n",
    "    (0.885579, -1.506996e-06),\n",
    "    (0.880119, 8.620770e-06),\n",
    "    (0.851734, -1.284692e-06),\n",
    "    (0.864334, -7.438977e-08),\n",
    "    (0.879546, -7.403128e-06),\n",
    "    (0.862461, 9.244535e-07)\n",
    "]\n",
    "for i, (_, df2_idx, model_name, new_col) in enumerate(model_param_pairs):\n",
    "    rho_print, k_print = param_print_format[i]\n",
    "    print(f\"‚úì Model {i+1}/6 {model_name} - Independent parameters: œÅ={rho_print:.6f}, k={k_print:.6e} ‚Üí New column: {new_col}\")\n",
    "\n",
    "# Validate initial stock columns\n",
    "init_stock_cols = [pair[0][2] for pair in model_param_pairs]\n",
    "for col in init_stock_cols:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Initial stock column {col} does not exist\")\n",
    "print(f\"\\nInitial dataset shape: ({df.shape[0]}, {df.shape[1]}) (contains {df.shape[1]} columns)\")\n",
    "\n",
    "# State space model fitting function\n",
    "def fit_state_space_model(group, stock_var, posting_share_lag_var, init_stock_var, target_rho, target_kappa):\n",
    "    group = group.sort_values('year_quarter').reset_index(drop=True)\n",
    "    rcid = group['rcid'].iloc[0]\n",
    "    init_stock = group[init_stock_var].iloc[0]\n",
    "    \n",
    "    if len(group) < 2:\n",
    "        print(f\"Note: Firm {rcid} time series length={len(group)}, fill with initial stock\")\n",
    "        group['temp_latent_var'] = init_stock\n",
    "        return group\n",
    "    \n",
    "    y = group[stock_var].dropna().values\n",
    "    X = group[posting_share_lag_var].dropna().values\n",
    "    \n",
    "    if len(y) < 2 or len(X) < 2:\n",
    "        print(f\"Note: Firm {rcid} insufficient valid samples, fill with initial stock\")\n",
    "        group['temp_latent_var'] = init_stock\n",
    "        return group\n",
    "    \n",
    "    X = X.reshape(-1, 1)\n",
    "    \n",
    "    try:\n",
    "        base_model = sarimax.SARIMAX(\n",
    "            endog=y,\n",
    "            exog=X,\n",
    "            order=(1, 0, 0),\n",
    "            trend='c',\n",
    "            enforce_stationarity=True,\n",
    "            enforce_invertibility=False,\n",
    "            missing='drop'\n",
    "        )\n",
    "        \n",
    "        params = base_model.start_params\n",
    "        param_names = base_model.param_names\n",
    "        \n",
    "        for i, name in enumerate(param_names):\n",
    "            if 'ar.L1' in name:\n",
    "                params[i] = target_rho\n",
    "            elif 'x1' in name:\n",
    "                params[i] = target_kappa\n",
    "            elif 'intercept' in name:\n",
    "                params[i] = 0.0\n",
    "        \n",
    "        results = base_model.smooth(params, transformed=False)\n",
    "        \n",
    "        latent_states = results.filtered_state[0]\n",
    "        latent_series = pd.Series(index=group.index, dtype=float)\n",
    "        valid_idx = group[stock_var].notna().index\n",
    "        latent_series.loc[valid_idx[:len(latent_states)]] = latent_states\n",
    "        latent_series = latent_series.fillna(init_stock)\n",
    "        \n",
    "        group['temp_latent_var'] = latent_series\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)[:50]\n",
    "        print(f\"Warning: Firm {rcid} fitting failed, fill with initial stock, error: {error_msg}\")\n",
    "        group['temp_latent_var'] = init_stock\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Batch fit state space models (keep all firms)\n",
    "final_df = df.copy(deep=True)\n",
    "unique_rcids = final_df['rcid'].unique()\n",
    "print(f\"\\nTotal number of firms for fitting: {len(unique_rcids)} (keep all firms)\")\n",
    "\n",
    "for idx, ((stock_var, posting_lag_var, init_stock_var), df2_idx, model_name, new_col) in enumerate(model_param_pairs):\n",
    "    print(f\"\\n=== Fitting Model {idx+1}/6: Generate new column {new_col} ({model_name}) ===\")\n",
    "    target_rho = df2.iloc[df2_idx]['rho_hat']\n",
    "    target_kappa = df2.iloc[df2_idx]['kappa_hat']\n",
    "\n",
    "    current_results = []\n",
    "    for rcid in tqdm(unique_rcids, desc=f\"Model {idx+1} progress\", position=0, leave=True):\n",
    "        group = final_df[final_df['rcid'] == rcid].copy()\n",
    "        processed_group = fit_state_space_model(\n",
    "            group=group,\n",
    "            stock_var=stock_var,\n",
    "            posting_share_lag_var=posting_lag_var,\n",
    "            init_stock_var=init_stock_var,\n",
    "            target_rho=target_rho,\n",
    "            target_kappa=target_kappa\n",
    "        )\n",
    "        current_results.append(processed_group)\n",
    "\n",
    "    # Merge results (keep all rows)\n",
    "    temp_df = pd.concat(current_results, ignore_index=True)[['rcid', 'year_quarter', 'temp_latent_var']]\n",
    "    final_df = final_df.merge(temp_df, on=['rcid', 'year_quarter'], how='left', suffixes=('', '_drop'))\n",
    "    # Assign new column, fill missing values with initial stock\n",
    "    final_df[new_col] = final_df['temp_latent_var'].fillna(final_df[init_stock_var])\n",
    "    # Clean temporary columns\n",
    "    final_df = final_df.drop(columns=[col for col in final_df.columns if col.endswith('_drop') or col == 'temp_latent_var'])\n",
    "    final_df = final_df.drop_duplicates(subset=['rcid', 'year_quarter']).reset_index(drop=True)\n",
    "\n",
    "# Save and validate results\n",
    "save_path = \"output.parquet\"\n",
    "final_df.to_parquet(save_path, index=False)\n",
    "\n",
    "print(f\"All models fitted successfully (all firms retained)!\")\n",
    "print(f\"Final dataset shape: {final_df.shape} (original {df.shape[1]} columns + 6 new columns = {len(final_df.columns)} columns)\")\n",
    "print(f\"Complete dataset saved to: {save_path}\")\n",
    "print(f\"Stock columns added:\")\n",
    "for _, _, _, new_col in model_param_pairs:\n",
    "    print(f\"   - {new_col}\")\n",
    "\n",
    "# Validate new column value distribution\n",
    "print(\"\\nüìà New column value distribution validation:\")\n",
    "for _, _, _, new_col in model_param_pairs:\n",
    "    zero_count = (final_df[new_col] == 0).sum()\n",
    "    non_zero_count = len(final_df) - zero_count\n",
    "    unique_rcid = final_df[final_df[new_col].notna()]['rcid'].nunique()\n",
    "    print(f\"{new_col}:\")\n",
    "    print(f\"  - Zero values: {zero_count} ({zero_count/len(final_df)*100:.2f}%)\")\n",
    "    print(f\"  - Non-zero values: {non_zero_count} ({non_zero_count/len(final_df)*100:.2f}%)\")\n",
    "    print(f\"  - Covered firms: {unique_rcid}/{len(unique_rcids)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep top 50% firms by AIC_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('import.parquet')\n",
    "\n",
    "# Sort by AIC_stock_it_2 (descending)\n",
    "df_sorted = df.sort_values(by='AIC_stock_it_2_œÅk', ascending=False)\n",
    "\n",
    "# Get top 50% firm rcids\n",
    "half_length = int(len(df_sorted['rcid'].unique()) * 0.5)\n",
    "top_50_percent_rcid = df_sorted['rcid'].unique()[:half_length]\n",
    "\n",
    "# Filter data (keep 44 quarters per firm)\n",
    "df_filtered = df[df['rcid'].isin(top_50_percent_rcid)]\n",
    "\n",
    "# Validate filtered data\n",
    "filtered_unique_enterprises = df_filtered['rcid'].nunique()\n",
    "filtered_data_length = df_filtered.shape[0]\n",
    "print(f\"Number of firms after filtering: {filtered_unique_enterprises}\")\n",
    "print(f\"Number of rows after filtering: {filtered_data_length}\")\n",
    "\n",
    "# Save filtered dataset\n",
    "df_filtered.to_parquet('output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate KPSS patent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_parquet('import.parquet')\n",
    "\n",
    "# Remove duplicate patents\n",
    "df1_unique = df1.drop_duplicates(subset=['patent_num'])\n",
    "\n",
    "# Define aggregation rules\n",
    "agg_dict = {\n",
    "    'xi_real': 'sum',\n",
    "    'cites': 'sum',\n",
    "    'patent_num': 'nunique'\n",
    "}\n",
    "\n",
    "# Filter AI patent data (is_aicpc=1)\n",
    "df1_ai = df1_unique[df1_unique['is_aicpc'] == 1]\n",
    "\n",
    "# Aggregate by filing quarter (_f suffix)\n",
    "agg_f = df1_unique.groupby(['rcid', 'issue_year_quarter']).agg(agg_dict).reset_index()\n",
    "agg_f.columns = ['rcid', 'year_quarter_f', 'real_f', 'cites_f', 'patent_count_f']\n",
    "\n",
    "ai_agg_f = df1_ai.groupby(['rcid', 'issue_year_quarter']).agg(agg_dict).reset_index()\n",
    "ai_agg_f.columns = ['rcid', 'year_quarter_f', 'ai_real_f', 'ai_cites_f', 'ai_patent_count_f']\n",
    "\n",
    "agg_f_merged = pd.merge(agg_f, ai_agg_f, on=['rcid', 'year_quarter_f'], how='left').fillna(0)\n",
    "\n",
    "# Aggregate by issue quarter (_i suffix)\n",
    "agg_i = df1_unique.groupby(['rcid', 'filing_year_quarter']).agg(agg_dict).reset_index()\n",
    "agg_i.columns = ['rcid', 'year_quarter_i', 'real_i', 'cites_i', 'patent_count_i']\n",
    "\n",
    "ai_agg_i = df1_ai.groupby(['rcid', 'filing_year_quarter']).agg(agg_dict).reset_index()\n",
    "ai_agg_i.columns = ['rcid', 'year_quarter_i', 'ai_real_i', 'ai_cites_i', 'ai_patent_count_i']\n",
    "\n",
    "agg_i_merged = pd.merge(agg_i, ai_agg_i, on=['rcid', 'year_quarter_i'], how='left').fillna(0)\n",
    "\n",
    "# Create complete quarterly panel (2013-01 to 2023-10)\n",
    "quarters = [\n",
    "    f'{y}-01' for y in range(2013, 2024)\n",
    "] + [\n",
    "    f'{y}-04' for y in range(2013, 2024)\n",
    "] + [\n",
    "    f'{y}-07' for y in range(2013, 2024)\n",
    "] + [\n",
    "    f'{y}-10' for y in range(2013, 2024)\n",
    "]\n",
    "target_quarters = [q for q in sorted(list(set(quarters))) if q >= '2013-01' and q <= '2023-10']\n",
    "target_quarters = sorted(target_quarters)\n",
    "\n",
    "unique_rcid = df1['rcid'].unique()\n",
    "rcid_quarter = pd.MultiIndex.from_product([unique_rcid, target_quarters], names=['rcid', 'year_quarter']).to_frame(index=False)\n",
    "\n",
    "# Merge aggregated results to complete panel\n",
    "df1 = pd.merge(rcid_quarter, agg_f_merged, left_on=['rcid', 'year_quarter'], right_on=['rcid', 'year_quarter_f'], how='left')\n",
    "df1 = pd.merge(df1, agg_i_merged, left_on=['rcid', 'year_quarter'], right_on=['rcid', 'year_quarter_i'], how='left')\n",
    "\n",
    "# Clean columns and fill missing values with 0\n",
    "df1 = df1.drop(columns=['year_quarter_f', 'year_quarter_i'], errors='ignore')\n",
    "all_target_cols = [\n",
    "    'real_f', 'cites_f', 'patent_count_f',\n",
    "    'real_i', 'cites_i', 'patent_count_i',\n",
    "    'ai_real_f', 'ai_cites_f', 'ai_patent_count_f',\n",
    "    'ai_real_i', 'ai_cites_i', 'ai_patent_count_i'\n",
    "]\n",
    "for col in all_target_cols:\n",
    "    df1[col] = df1[col].fillna(0)\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1.to_parquet('output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Compustat firm financial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('import.csv')\n",
    "\n",
    "# Calculate R&D intensity (xrdintensity = xrdq / saleq)\n",
    "df1['xrdintensity'] = df1['xrdq'] / df1['saleq']\n",
    "df1.loc[(df1['saleq'] == 0) | (df1['saleq'].isna()), 'xrdintensity'] = pd.NA\n",
    "df1.loc[(df1['xrdq'].isna()) & (df1['saleq'] != 0) & (~df1['saleq'].isna()), 'xrdintensity'] = 0\n",
    "\n",
    "# Calculate ROA (roa = niq / atq)\n",
    "df1['roa'] = df1['niq'] / df1['atq']\n",
    "\n",
    "# Calculate Tobin's Q\n",
    "df1['tobinQ'] = (df1['atq'] + (df1['cshoq'] * df1['prccq']) - df1['ceqq']) / df1['atq']\n",
    "\n",
    "# Calculate cash holding ratio (cash_at = cheq / atq)\n",
    "df1['cash_at'] = df1['cheq'] / df1['atq']\n",
    "df1.loc[(df1['atq'] == 0) | (df1['atq'].isna()), 'cash_at'] = pd.NA\n",
    "\n",
    "# Calculate leverage (leverage = (dlttq + dlcq) / atq)\n",
    "df1['dlttq_filled'] = df1['dlttq'].fillna(0)\n",
    "df1['dlcq_filled'] = df1['dlcq'].fillna(0)\n",
    "df1['leverage'] = (df1['dlttq_filled'] + df1['dlcq_filled']) / df1['atq']\n",
    "df1.loc[(df1['atq'] == 0) | (df1['atq'].isna()), 'leverage'] = pd.NA\n",
    "df1.drop(columns=['dlttq_filled', 'dlcq_filled'], inplace=True)\n",
    "\n",
    "# Calculate firm age (age = current_year - ipo_year)\n",
    "df1['current_year'] = df1['year_quarter'].str.split('-').str[0].astype(int)\n",
    "df1['ipo_year'] = pd.NA\n",
    "df1['ipodate_str'] = df1['ipodate'].astype(str)\n",
    "ipo_year_format1 = df1['ipodate_str'].str.split('-').str[0]\n",
    "ipo_year_format2 = df1['ipodate_str'].str[:4]\n",
    "df1['ipo_year'] = pd.to_numeric(ipo_year_format1.where(ipo_year_format1.str.len() == 4, ipo_year_format2), errors='coerce')\n",
    "df1.drop(columns=['ipodate_str'], inplace=True)\n",
    "df1['age'] = df1['current_year'] - df1['ipo_year']\n",
    "df1.loc[(df1['current_year'].isna()) | (df1['ipo_year'].isna()), 'age'] = pd.NA\n",
    "\n",
    "df1.to_parquet('output.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate grouping variables for heterogeneity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('import.csv')\n",
    "\n",
    "# Keep core columns\n",
    "core_columns = [\n",
    "    'rcid', 'year_quarter',\n",
    "    'xsgaq', 'saleq',\n",
    "    'intanq', 'atq'\n",
    "]\n",
    "df1 = df1[[col for col in core_columns if col in df1.columns]]\n",
    "\n",
    "# Calculate SG&A expense intensity (H1)\n",
    "df1['xsgaq'] = df1['xsgaq'].fillna(0)\n",
    "df1['saleq'] = df1['saleq'].fillna(0)\n",
    "df1['sgna_intensity'] = np.where(\n",
    "    df1['saleq'] > 0,\n",
    "    df1['xsgaq'] / df1['saleq'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate intangible asset intensity (H2)\n",
    "df1['intanq'] = df1['intanq'].fillna(0)\n",
    "df1['atq'] = df1['atq'].fillna(0)\n",
    "df1['intan_intensity'] = np.where(\n",
    "    df1['atq'] > 0,\n",
    "    df1['intanq'] / df1['atq'],\n",
    "    0\n",
    ")\n",
    "\n",
    "final_columns = [\n",
    "    'rcid', 'year_quarter',\n",
    "    'sgna_intensity', 'intan_intensity'\n",
    "]\n",
    "df1 = df1[final_columns]\n",
    "\n",
    "df1.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('import1.csv')\n",
    "df2 = pd.read_csv('import2.csv')\n",
    "\n",
    "# Merge datasets\n",
    "df1 = pd.merge(df1, df2, on=['rcid', 'year_quarter'], how='left')\n",
    "\n",
    "# Fill missing values with 0\n",
    "df1['sgna_intensity'] = df1['sgna_intensity'].fillna(0)\n",
    "df1['intan_intensity'] = df1['intan_intensity'].fillna(0)\n",
    "\n",
    "# Calculate mean by rcid\n",
    "sgna_mean = df1.groupby('rcid')['sgna_intensity'].mean().reset_index()\n",
    "sgna_mean.rename(columns={'sgna_intensity': 'sgna_intensity_mean'}, inplace=True)\n",
    "\n",
    "intan_mean = df1.groupby('rcid')['intan_intensity'].mean().reset_index()\n",
    "intan_mean.rename(columns={'intan_intensity': 'intan_intensity_mean'}, inplace=True)\n",
    "\n",
    "# Merge mean columns back to df1\n",
    "df1 = pd.merge(df1, sgna_mean, on='rcid', how='left')\n",
    "df1 = pd.merge(df1, intan_mean, on='rcid', how='left')\n",
    "\n",
    "# Initialize binary label columns\n",
    "df1['sgna_intensity_binary'] = np.nan\n",
    "df1['intan_intensity_binary'] = np.nan\n",
    "\n",
    "# Filter valid rcids (non-null patent_count_f)\n",
    "valid_mask = df1['patent_count_f'].notnull()\n",
    "valid_rcids = df1.loc[valid_mask, 'rcid'].unique()\n",
    "\n",
    "# Calculate median of mean values for valid rcids\n",
    "mean_data = df1.loc[df1['rcid'].isin(valid_rcids), ['rcid', 'sgna_intensity_mean', 'intan_intensity_mean']].drop_duplicates(subset='rcid')\n",
    "sgna_median = mean_data['sgna_intensity_mean'].median() if not mean_data.empty else 0\n",
    "intan_median = mean_data['intan_intensity_mean'].median() if not mean_data.empty else 0\n",
    "\n",
    "print(f\"Median of sgna_intensity_mean for valid rcids: {sgna_median}\")\n",
    "print(f\"Median of intan_intensity_mean for valid rcids: {intan_median}\")\n",
    "\n",
    "# Binary labeling function (>= median = 1, < median = 0)\n",
    "def binary_by_median(x, median):\n",
    "    return 1 if x >= median else 0\n",
    "\n",
    "# Assign binary labels to valid rcids\n",
    "for rcid in valid_rcids:\n",
    "    try:\n",
    "        sgna_mean_val = mean_data.loc[mean_data['rcid'] == rcid, 'sgna_intensity_mean'].values[0]\n",
    "        intan_mean_val = mean_data.loc[mean_data['rcid'] == rcid, 'intan_intensity_mean'].values[0]\n",
    "    except IndexError:\n",
    "        sgna_mean_val = 0\n",
    "        intan_mean_val = 0\n",
    "    \n",
    "    sgna_label = binary_by_median(sgna_mean_val, sgna_median)\n",
    "    intan_label = binary_by_median(intan_mean_val, intan_median)\n",
    "    \n",
    "    df1.loc[df1['rcid'] == rcid, 'sgna_intensity_binary'] = sgna_label\n",
    "    df1.loc[df1['rcid'] == rcid, 'intan_intensity_binary'] = intan_label\n",
    "\n",
    "# Count unique rcids by binary labels\n",
    "valid_rcid_labels = df1.loc[df1['rcid'].isin(valid_rcids), ['rcid', 'sgna_intensity_binary', 'intan_intensity_binary']].drop_duplicates(subset='rcid')\n",
    "sgna_count = valid_rcid_labels['sgna_intensity_binary'].value_counts(dropna=False)\n",
    "intan_count = valid_rcid_labels['intan_intensity_binary'].value_counts(dropna=False)\n",
    "\n",
    "print(\"\\n=== Number of unique rcids by sgna_intensity_binary (0/1) ===\")\n",
    "print(f\"0 (below median): {sgna_count.get(0.0, 0)}\")\n",
    "print(f\"1 (above or equal to median): {sgna_count.get(1.0, 0)}\")\n",
    "print(f\"Unlabeled (invalid rcids): {sgna_count.get(np.nan, 0)} (Note: rcids with missing patent_count_f)\")\n",
    "\n",
    "print(\"\\n=== Number of unique rcids by intan_intensity_binary (0/1) ===\")\n",
    "print(f\"0 (below median): {intan_count.get(0.0, 0)}\")\n",
    "print(f\"1 (above or equal to median): {intan_count.get(1.0, 0)}\")\n",
    "print(f\"Unlabeled (invalid rcids): {intan_count.get(np.nan, 0)} (Note: rcids with missing patent_count_f)\")\n",
    "\n",
    "df1.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
